## Автоматическое применение правил хранения к таблицам на движке GraphiteMergeTree в DBMS ClickHouse

Приветствую, habr. Если кто-то эксплуатирует такую систему, как [graphite-web](https://github.com/graphite-project/graphite-web) и столкнулся с проблемой производительности хранилища [whisper](https://github.com/graphite-project/whisper) (IO, потребляемое дисковое пространство), то шанс того, что был брошен взгляд на ClickHouse в качестве замены, по моей оценке должен быть выше 60%. Данное утверждение подразумевает, что в качестве принимающего демона уже используется сторонняя реализация, например [carbonwriter](https://github.com/grobian/carbonwriter) или [go-carbon](https://github.com/go-graphite/go-carbon).

ClickHouse хорошо решает описанные проблемы. Например, после переливки 2TiB данных из whisper, они уместились в 300GiB. Подробно на сравнении я останавливаться не буду, потому что до недавнего времени с нашим ClickHouse хранилищем тоже было не всё гладко.
<cut text="Так что же не так?"/>

### Так что же не так?

На первый взгляд, всё должно работать хорошо. Мы, следуя [документации](https://clickhouse.yandex/docs/ru/operations/table_engines/graphitemergetree/#primer-konfiguratsii), создаём конфиг для схемы хранения метрик (далее `retention`), затем создаём таблицу согласно рекомендации выбранного бекенда для graphite-web: [carbon-clickhouse](https://github.com/lomik/carbon-clickhouse)+[graphite-clickhouse](https://github.com/lomik/graphite-clickhouse) или [graphouse](https://github.com/ClickHouse/graphouse), в зависимости от того, какой стек Вы используете. И далее - ловушка.

Для того, чтобы это понять, какая, надо знать, как работают вставки и дальнейший жизненный путь данных в таблицах движков семейства *\*MergeTree* ClickHouse:

* Вставляется `батч` данных. В нашем случае это прилетели метрики.
* Каждый такой `блок` данных перед записью на диск сортируется согласно ключу `ORDER BY`, указанному при создании таблицы.
* После сортировки, `кусок` (`part`) данных записывается на диск.
* Сервер следит в фоне, чтобы таких кусков было не много и запускает фоновые `слияния` (`merge`, далее мержи).
* Сервер перестаёт запускать мержи самостоятельно, как только данные перестают активно поступать в `партицию` (`partition`), но можно запустить merge вручную командой `OPTIMIZE`.
* Если в партиции остался только один кусок, то запустить мерж обычной командой не получится, необходимо использовать `OPTIMIZE ... FINAL`

Итак, поступают первые метрики. И они занимают некое пространство. Последующие события могут несколько варьироваться в зависимости от многих факторов: 

* Ключ партиционирования может быть как очень маленьким (день), так и очень большим (несколько месяцев)
* Конфиг retention может умещать несколько значительных порогов агрегации данных внутри активной партиции (куда идёт запись метрик), а может и нет
* Если данных очень много, то самые ранние куски, которые из-за фоновых мержей могут уже быть огромными (при выборе неоптимального ключа партиционирования), не будут мержиться сами со свежими маленькими кусками

![merge](https://habrastorage.org/webt/zu/qt/b9/zuqtb90bcok18z_qs7pw1zxd88g.jpeg)

И заканчивается всегда всё одинаково. Место, занимаемое метриками в ClickHouse только растёт, если не:

1. Применять `OPTIMIZE ... FINAL` вручную или
2. Не вставлять данные во все партиции на постоянной основе, чтобы рано или поздно запустить фоновый мерж

Второй способ кажется наиболее простым в реализации ~~и, значит, он неправильный~~ и был опробован в первую очередь.  
Я написал достаточно простой скрипт на питоне, который отправлял фиктивные метрики для каждого дня за прошлые 4 года и запускался каждый час кроном.  
Так как вся работа ClickHouse DBMS построена на том, что эта система рано или поздно сделает всю фоновую работу, но неизвестно когда, то дождаться момента, когда старые огромные куски соизволят начать мерж с новыми маленькими, мне не удалось (картинка с собачками ещё раз). Стало ясно, что надо искать способ автоматизировать принудительные оптимизации.

### Какая информация есть в ClickHouse?

Этот вопрос я задал себе. Взглянем на структуру таблицы [system.parts](https://clickhouse.yandex/docs/ru/operations/system_tables/#system_tables-parts). Это исчерпывающая информация о каждом куске всех таблиц на сервере ClickHouse. И здесь имеются, в том числе, следующие столбцы:

* Имя БД (`database`)
* Имя таблицы (`table`)
* Имя и ИД партиции (`partition` & `partition_id`)
* Когда кусок был создан (`modification_time`)
* Минимальная и максимальная дата в куске (партиционирование идёт по дням) (`min_date` & `max_date`)

Также имеется таблица [system.graphite_retentions](https://clickhouse.yandex/docs/ru/operations/system_tables/#system-graphite-retentions), со следующими интересными полями:

* Имя БД (`Tables.database`)
* Имя таблицы (`Tables.table`)
* Возраст метрики, когда должна быть применена следующая агрегация (`age`)

Внимательному читателю всё уже понятно и, наверное, дальше читать уже скучно, для меня же эта задачка была первой пробой джойнов и отчаянной попыткой в конце концов разобраться в них.  
В ретроспективе всё кажется элементарным, на разработку же запроса было потрачено несколько литров пива и несколько админо-дней совместно с @v0devil, за что я ему безумно благодарен.

Итак:

1. У нас имеется таблица кусков и таблица правил агрегации
1. Объединяем их пересечение и получаем все таблицы \*GraphiteMergeTree
1. Ищем все партиции, в которых:
    - больше одного куска
    - или настал момент применить следующее правило агрегации, и `modification_time` старше этого момента

### Реализация

<spoiler title="Данный запрос">

```sql
SELECT
    concat(p.database, '.', p.table) AS table,
    p.partition_id AS partition_id,
    p.partition AS partition,
    max(g.age) AS age,
    countDistinct(p.name) AS parts,
    toDateTime(max(p.max_date + 1)) AS max_time,
    max_time + age AS rollup_time,
    min(p.modification_time) AS modified_at
FROM system.parts AS p
INNER JOIN
(
    SELECT
        Tables.database AS database,
        Tables.table AS table,
        age
    FROM system.graphite_retentions
    ARRAY JOIN Tables
    GROUP BY
        database,
        table,
        age
) AS g ON (p.table = g.table) AND (p.database = g.database)
WHERE p.active AND ((toDateTime(p.max_date + 1) + g.age) < now())
GROUP BY
    table,
    partition
HAVING (modified_at < rollup_time) OR (parts > 1)
ORDER BY
    table ASC,
    partition ASC,
    age ASC
```

</spoiler>

возвращает каждую партицию в таблиц \*GraphiteMergeTree, мерж которых должен привести к увеличению свободного места на диске. Осталось только дело за малым: пройтись по ним всем с запросом `OPTIMIZE ... FINAL`

Именно это и делает проект [graphite-ch-optimizer](https://github.com/innogames/graphite-ch-optimizer), который больше месяца назад был опубликован на github. Бывшие коллеги из Яндекс.Маркет опробовали его в продакшене, результат работы можно видеть ниже.

![result](https://habrastorage.org/webt/23/wv/mw/23wvmwkqw9ckohfvfqfshwhbqmu.jpeg)

В ближайших планах - предоставить, по крайней мере, deb пакеты, а по возможности - ещё и rpm.

### Вместо заключения

За прошедшие 9 с лишним месяцев я внутри своей компании InnoGames провёл много времени, варясь на стыке ClickHouse и graphite-web. Это был хороший опыт, результатом которого стал возможен скорый переход с whisper на ClickHouse в качестве хранилища метрик. Надеюсь, что эта статья - что-то вроде начала цикла о том, какие улучшения были внесены нами в различные части этого стека и что будет сделано в будущем.
